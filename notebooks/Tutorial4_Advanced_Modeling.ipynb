{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d95e16-2a1b-44bf-8a78-d697fed20da3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial 4: Advanced modeling\n",
    "\n",
    "After preparing a basic model with the features from audio data and gaining an understanding of the metrics for this challenge it's time to use a more state-of-the-art approach. The model we are about to use is bigger and demands more computation power. The purpose of this tutorial is to help you understand the code that we will be using in the next notebook where we will fine-tune the model on the whole dataset. As we want only to get familiar with the model and its preprocessing, we will use a small sample of data in this notebook.  So if the results here seem to be discouraging please be patient, you will see it will pay off in the next part of the tutorials.\n",
    "\n",
    "For this notebook, we'll use the [ðŸ¤— (Hugging face) library](https://huggingface.co/). With its help, we will create the train, validation, and test datasets, preprocess the data, load a pre-trained model, and fine-tune it. By the end of this notebook, you will have a basic understanding of the huggingface library and you will be able to train a much more powerful model than Random Forest.\n",
    "\n",
    "For those of you who encounter the ðŸ¤— library for the first time let's ask ChatGPT for a short explanation of what it is:\n",
    ">\"Hugging Face is an open-source software library for natural language processing (NLP) tasks, such as text classification, machine translation, and question-answering. It provides easy-to-use interfaces to pre-trained language models, including state-of-the-art models such as GPT-3 and BERT, allowing developers to quickly build and deploy NLP applications. The library also includes a range of tools for fine-tuning and adapting pre-trained models to specific NLP tasks, as well as for training new models from scratch. Hugging Face has become popular in the NLP community due to its ease of use, flexibility, and strong community support. It is widely used in industry and academia for a variety of NLP applications.\"\n",
    "\n",
    "Hmm... that's a good start, but honestly, a more accurate explanation would include information that apart from NLP-oriented models the library provides state-of-the-art solutions for a variety of different Data Science tasks, such as Computer Vision, Reinforcement Learning aaaaand **Audio Processing**!\n",
    "\n",
    "Nevertheless, backed up by the ChatGPT promise of ðŸ¤— simplicity let's try to use the library to create an audio dataset, preprocess it, and finally fine-tune one of many available models on our classification task. The list of all available audio classifiers can be found [here](https://huggingface.co/models?pipeline_tag=audio-classification&sort=downloads), for starters we'll try to use one of the most popular ones - [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#audio-spectrogram-transformer).\n",
    "\n",
    "For those of you who were not convinced by ChatGPT's assurance of ðŸ¤— simplicity and want to have a good understanding of the library first here is an excellent introductory [course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt) that can help you with your first steps.\n",
    "\n",
    "But don't be scared, this notebook will guide you through the basic concepts of the library, so that at the end you will have ready your first hugging face model. Let's get started!\n",
    "\n",
    "**Note: For this notebook, you do need a GPU instance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b034b3-54af-4256-8757-789282e8984f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to import required libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1e3ca-df58-418e-8b76-770c30dfcefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#line to render the plots under the code cell that created it\n",
    "%matplotlib inline\n",
    "import json  # for working with json files\n",
    "import sys  # Python system library needed to load custom functions\n",
    "import numpy as np  # for performing calculations on numerical arrays\n",
    "import pandas as pd  # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import torch  # library to work with PyTorch tensors and to figure out if we have a GPU available\n",
    "import os     # for changing the directory\n",
    "\n",
    "from datasets import load_dataset, Audio  # required tools to create, load and process our audio dataset\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, TrainingArguments, Trainer  # required classes to perform the model training\n",
    "\n",
    "sys.path.append('../src')  # add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "from gdsc_utils import download_directory, PROJECT_DIR # function to download the needed files from the official GDSC s3 bucket and our root directory\n",
    "from config import DEFAULT_BUCKET  # S3 bucket with the GDSC data\n",
    "from preprocessing import calculate_stats, preprocess_audio_arrays  # functions to calculate dataset statistics and preprocess the dataset with ASTFeatureExtractor\n",
    "from gdsc_eval import make_predictions, compute_metrics  # functions to create predictions and evaluate them\n",
    "os.chdir(PROJECT_DIR) # changing our directory to root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058d09e-a8e1-49ad-a0f6-125b775ac4dd",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad9f24-cb3d-4afa-b8c4-f31ca4646d6c",
   "metadata": {},
   "source": [
    "Next we need to download the official data for the GDSC from the S3 bucket. The S3 bucket is structured as follows:\n",
    "\n",
    "```\n",
    "S3_bucket/\n",
    "    â””â”€â”€ data/\n",
    "        |â”€â”€ labels.json\n",
    "        â””â”€â”€ train/\n",
    "            |â”€â”€ train_file_1.wav\n",
    "            |â”€â”€ train_file_2.wav\n",
    "            |â”€â”€ ...\n",
    "            |â”€â”€ metadata.csv\n",
    "        â””â”€â”€ val/\n",
    "            |â”€â”€ val_file_1.wav\n",
    "            |â”€â”€ val_file_2.wav\n",
    "            |â”€â”€ ...\n",
    "            |â”€â”€ metadata.csv\n",
    "        â””â”€â”€ test/\n",
    "            |â”€â”€ test_file_1.wav\n",
    "            |â”€â”€ test_file_2.wav\n",
    "            |â”€â”€ ...\n",
    "            |â”€â”€ metadata.csv\n",
    "    â””â”€â”€ data_small/\n",
    "        |â”€â”€ labels.json\n",
    "        â””â”€â”€ train/\n",
    "            |â”€â”€ train_file_1.wav\n",
    "            |â”€â”€ train_file_2.wav\n",
    "            |â”€â”€ ...\n",
    "            |â”€â”€ metadata.csv\n",
    "        â””â”€â”€ val/\n",
    "            |â”€â”€ val_file_1.wav\n",
    "            |â”€â”€ val_file_2.wav\n",
    "            |â”€â”€ ...\n",
    "            |â”€â”€ metadata.csv\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ecc70-613f-402c-91db-af8b13bc0d5b",
   "metadata": {},
   "source": [
    "In the official S3 bucket, you can find 2 folders:\n",
    "\n",
    "- *data* - it contains the complete dataset for the challenge. We already downloaded it in the 2nd tutorial (EDA).\n",
    "- *data_small* - this folder contains a small sample of the training and validation datasets.\n",
    "\n",
    "For the purpuse of this tutorial we need to download *data_small* directory. We can make use of the ```download_directory``` function to accomplish that. Let's store in our *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd4d01-ed2e-46dc-a288-a0b5333f071e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_directory('data_small/', 'data', DEFAULT_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb031d-d36b-4577-b0ed-25707ddaba11",
   "metadata": {},
   "source": [
    "## Creating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ee42b-b3ac-40d4-a5e8-05a18b809b3d",
   "metadata": {},
   "source": [
    "After having imported the required libraries it's about time to create a ðŸ¤— dataset object that will allow us to handle our audio files during preprocessing and training. This will be also the first \"proof\" for ease of use of the ðŸ¤— library.\n",
    "\n",
    "The ðŸ¤— datasets module has a neat way to load the audio data type with which we are working. The only thing we need is the paths to the folders with audio and metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b951f5-590f-4c41-9f4b-e2957ab097f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paths for the train and validation datasets\n",
    "train_path = 'data/data_small/train'\n",
    "val_path = 'data/data_small/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b5fe9-e5e8-4dee-98e8-efeadde5e95f",
   "metadata": {},
   "source": [
    "Let's see what is the structure of the metadata files stored in those paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372700d0-cb3b-4b53-9d26-45bf8f5f07c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_meta_df = pd.read_csv(f\"{train_path}/metadata.csv\")\n",
    "val_meta_df = pd.read_csv(f\"{val_path}/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68902c04-0236-4d1e-96eb-85a87ebec1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9e697-5058-4f1b-a03f-6167f0a76430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175590f-ad86-4359-bdd3-8bb26c82272f",
   "metadata": {},
   "source": [
    "The files with metadata have a simple structure - they consist only of two columns: *file_name* and *label*.\n",
    "\n",
    "Once we establish where ðŸ¤— needs to look for the audio files and their metadata we can use the *load_dataset* function and create an AudioFolder object which is designed to work with audio data. We encourage you to read more about the AudioFolder builder [here](https://huggingface.co/docs/datasets/audio_load#audiofolder) and [here](https://huggingface.co/docs/datasets/audio_dataset#audiofolder).\n",
    "\n",
    "We will also use the shuffle method on the train set to avoid inputting sorted data points to our model, which might negatively affect its convergence. We use a random seed of 42, to ensure the reproducibility of the output. This also allows you to cache the dataset, so that you can load it without the need of recomputing the shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e836316-808a-44cd-a311-cdeec4a8c7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our first interaction with Hugging Face datasets!\n",
    "train_dataset = load_dataset(\"audiofolder\", data_dir=train_path).get('train').shuffle(seed = 42)  # load the dataset and shuffle the examples\n",
    "val_dataset = load_dataset(\"audiofolder\", data_dir=val_path).get('train')                         # load the validation dataset. But why do we have \"get('train')\" at the end of the line? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865a1db-d193-4c41-a90e-ea8822797afc",
   "metadata": {},
   "source": [
    "Seems that the dataset was loaded. Let's inspect the train_dataset and val_dataset variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0826f6b-e262-44a7-81b9-6bd9b297936b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83cd29-48c6-4010-a4fd-ae64115b181b",
   "metadata": {},
   "source": [
    "So clearly we've created some kind of dataset object. We can see that it has two features: 'audio' and 'label'. Let's see if we can unpack a bit more this vague looking object and see what exactly the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7a738-33de-4349-a385-0e59e0ac8178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0], val_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521320b-b815-4b2c-8830-e7021e979178",
   "metadata": {},
   "source": [
    "We can see the first record of the train and validation sets. \n",
    "\n",
    "The 'audio' feature consists of a dictionary with keys: \n",
    "1. *path* - the path to the file including the folders\n",
    "2. *array* - the loaded audio sample and consists of the amplitudes\n",
    "3. *sampling rate* - information about the number of points that make up a second of recording\n",
    "\n",
    "The label feature is a simple integer that denotes the class of the recorded species. If you wonder about what class is assigned to a given integer, please inspect the *labels.json* file in the *data* directory.\n",
    "\n",
    "Great! We've just completed the first step of creating a dataset in no time. Now we need to slightly preprocess the data, so that it will have the form required by the model that we are about to use. We've mentioned at the beginning that we will use the [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example). In the next section, we'll try to understand what we need to do with our dataset to successfully fine-tune the model.\n",
    "\n",
    "**Key insights:**\n",
    "* The ðŸ¤— datasets audio folder consists of two columns:\n",
    "    * audio - compound column with path, the amplitude array and the sampling rate\n",
    "    * label - an integer indicating the class of the file\n",
    "\n",
    "**Exercise time:**\n",
    "* Can you try to inspect the cell in which we load the dataset and try to figure out why we use the get method with a key equal to \"train\"?\n",
    "* What kind of object do we get without it?\n",
    "* What is this object? Inspect the ðŸ¤— documentation\n",
    "* Can we avoid using the 'train' field in the above cell? Post your findings on the Teams channel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f7861-cc6d-4352-91e1-610c63b56ac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636c836-f7d2-4a34-803f-13084f07ef50",
   "metadata": {},
   "source": [
    "As we've mentioned at the beginning we'll use the Audio Spectrogram Transformer (or AST for short) model, which was trained on a dataset called [AudioSet](https://research.google.com/audioset/). While this is the same type of data, it is very different from what we are working with. The AudioSet consists of clips from YouTube, which are nowhere close to insects' sound recordings. This is why we need to perform some preprocessing steps in order to reliably fine-tune the model for our purpose. Let's see what the ðŸ¤— [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#overview) of the AST model tells us.\n",
    "\n",
    "If we look closely at the *tips* section of the documentation, we'll learn that *\"itâ€™s recommended to take care of the input normalization (to make sure the input has a mean of 0 and std of 0.5). ASTFeatureExtractor takes care of this. Note that it uses the AudioSet mean and std by default\"*.\n",
    "\n",
    "This poses a first challenge - we need to calculate the mean and standard deviation of the input that we are going to plug into our model to use instead of the AudioSet stats. The input to the AST model is a spectrogram, so we need to calculate the stats not from the \"raw\" amplitude arrays that we've just loaded, but from their respective spectrograms. Luckily the *ASTFeatureExtractor* does just that - it extracts the features from the audio data that are needed for the model, which are the spectrogram arrays. Setting the *do_normalize* argument to *False* will return the spectrograms without performing the normalization on them, so that we can calculate the relevant stats.\n",
    "\n",
    "Okay, so we can start to code it, right? Well... not exactly yet. If we look at the [ASTFeatureExtractor documentation](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor) we will learn that the default sampling rate is set to 16 000 Hz. This happens to be the sampling rate of the AudioSet dataset on which the model was pre-trained. As we know from inspecting our dataset in the EDA phase and from the information displayed from looking at the first row of the train and validation sets our sampling rate is higher than that. In order to use the AST model we need first to resample our dataset to the sampling rate of 16 000 and only then calculate the statistics of such preprocessed audio set.\n",
    "\n",
    "The next cell takes care of resampling our data to the required sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e343b-6686-45d7-8e75-032f669bf736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_SAMPLING_RATE = 16000\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a5919-b573-4d8d-ac6f-e3a4be5ed735",
   "metadata": {},
   "source": [
    "Let's inspect the datasets in the below cell with the help of the info attribute of the Dataset object. We can see that the sampling rate for the *audio* column is set to 16 000, which is exactly what we wanted. Great! Seems that working with the ðŸ¤— library is really easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d5e8b-747e-4eeb-bae3-e228cdcf596f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset.info.features, val_dataset.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e78c9d-2fa3-4610-9db7-82e4a0903b77",
   "metadata": {},
   "source": [
    "Having the data resampled we can now load the feature extractor with the help of the *from_pretrained* method of the *ASTFeatureExtractor*. As we discussed above - we need to disable the normalization to get unnormalized spectrogram arrays on which we will calculate the required stats - mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b1e80-e32d-4a83-a8a8-d8a77ecbc0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor_stats = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", do_normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20697d80-7397-404c-ae1f-15efa135ad66",
   "metadata": {},
   "source": [
    "Now with the help of the *map* method of the ðŸ¤— dataset object we'll use the *calculate_stats* function from the preprocessing module that we have in the *src* directory to pass the audio data through our newly created feature extractor. \n",
    "\n",
    "For the *calculate_stats* function we need to pass the audio feature of our dataset, the names of the keys that will help us to extract only the array from the audio feature, and finally our feature extractor object. \n",
    "\n",
    "For the configuration of the map method, we set the *batched* argument to *True*, so that we won't process the data one by one. The default batch size is equal to 32, we do have some computational capacity, so we'll leave it unchanged. If you run this notebook on a smaller instance consider setting the batch_size argument to e.g. 2, to avoid out-of-memory issues. As we've said before: those two arguments ensure that you process a number of examples at once reducing a bit the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639555b-3aec-4834-bf6f-0651793d3791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x: calculate_stats(x, audio_field='audio', array_field='array', feature_extractor=feature_extractor_stats), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd928b54-c745-4c5a-ac98-8a9a6dea47a5",
   "metadata": {},
   "source": [
    "If we inspect the train_dataset object once again we will see that we've just created two more columns - *mean* and *std*. Those are the statistics for each file's spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7f7fb-688c-4f4f-9f9c-4dd966ffeddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdda3c-910f-4cb4-b07e-f008cdf47f91",
   "metadata": {},
   "source": [
    "Now the very last step to calculate the stats is to take the mean of the newly created columns. Those are the dataset statistics, that we will use for the model's feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adb67f-58a0-4649-8d6c-8b05c4c9aadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_mean = np.mean(train_dataset['mean'])\n",
    "dataset_std = np.mean(train_dataset['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeaf46a-fc5c-4822-81c7-38d315bf4bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_mean, dataset_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e40372-c02f-4359-9b65-4b66de01bc4e",
   "metadata": {},
   "source": [
    "As we won't need the mean and std columns in the next steps we can use the *remove_columns* method of the dataset object to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf12c56-5264-400c-a467-fcd1f9f0f039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73d529-28af-4eea-9e09-ade7e52dc3a2",
   "metadata": {},
   "source": [
    "Phew! We resampled the data and calculated the stats of the dataset that we are about to use. Now we need to do one last step in our data preprocessing journey - we need to once again instantiate the *ASTFeatureExtractor*, but this time we will pass the dataset stats and leave the default value of the *do_normalize* argument, which is *True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c74ec7-15f8-4e6e-b09d-7a79da334d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", mean=dataset_mean, std=dataset_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172ca32-8834-4527-b61d-498217e3d92a",
   "metadata": {},
   "source": [
    "Now let's once again invoke the *map* method of the ðŸ¤— dataset object on our train and validation sets, but this time we will use the *preprocess_audio_array_function* from our *preprocessing* module. We still stick to processing the dataset in batches, but this time we will also remove the *audio* column, as we won't need it any longer. The result of the below cell is a ready dataset that we can pass through the model in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba4f17-21ca-4e2c-ac7a-acb44f9fb6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_encoded = train_dataset.map(lambda x: preprocess_audio_arrays(x, audio_field='audio', \n",
    "                                                                            array_field='array', \n",
    "                                                                            feature_extractor=feature_extractor), remove_columns=\"audio\", batched=True, batch_size=2)\n",
    "val_dataset_encoded = val_dataset.map(lambda x: preprocess_audio_arrays(x, audio_field='audio', \n",
    "                                                                        array_field='array', \n",
    "                                                                        feature_extractor=feature_extractor), remove_columns=\"audio\", batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3c611-c4d5-49e2-a059-158d0e06dc9f",
   "metadata": {},
   "source": [
    "Let's inspect the two newly created datasets. Now we see two columns - *label*, which we already know, and *input_values*, which stores the spectrogram arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbd7e5-c0b2-4489-b300-0d5b53565c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_encoded, val_dataset_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ef434-ea2b-43dc-9b95-9fb0ab8f3401",
   "metadata": {},
   "source": [
    "**Key insights:**\n",
    "* The ðŸ¤— datasets object can be processed with the help of the map method. You can define if the processing should go in batches and if yes how many data points you wish to process at once\n",
    "* Some audio models may require a specific sampling rate of your data. Do remember to take care of that\n",
    "* The AST model requires us to pass the data through its feature extractor (ASTFeatureExtractor) - other models have their own extractors, so if you plan to implement other models (and we strongly encourage you to do so) remember to change the imports accordingly\n",
    "* The documentation advises us to pass the mean and standard deviation of our dataset - please remember to check the required preprocessing if you plan to use other models. Not doing so, may rob you of a higher score!\n",
    "\n",
    "**Exercise time:**\n",
    "* Can you inspect the input_values feature of the dataset? \n",
    "* What type of object does it store? What is the shape of it? \n",
    "* Can you figure out why the shape of one data point is equal to those numbers? *Hint*: try to inspect the [feature extractor parameters](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)!\n",
    "\n",
    "We finished preprocessing the data. Now we will focus on preparing the last bits for fine-tuning our model and finally run the training process for four epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c666f-8a10-4a12-a4e4-fdf9668399c9",
   "metadata": {},
   "source": [
    "# Fine-tuning the AST model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772ae3d-b10b-448d-85c5-7ffa59af55b9",
   "metadata": {},
   "source": [
    "The model that we are about to use was pretrained on the AudioSet dataset, which is very different from the data we are working with. That's why we need to make sure, that the model \"knows\" what are the classes that we want it to predict and how many of them do we have.\n",
    "\n",
    "In order to do this we will use our *labels.json* file which contains the mapping of the labels to integer ids. Let's load and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71295e0-2565-4f7f-8ae4-beb99bf74579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/labels.json', 'r') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fee10c-cfe0-43fd-aefe-36b43a9bc0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff1fe9-5c72-4956-81be-93650da4f374",
   "metadata": {},
   "source": [
    "From the above output, we can see, that basically, we are dealing with a Python dictionary. The keys are the names of the species and the values are integers from 0 to 65.\n",
    "\n",
    "Now we need to create one mapping of label to id and one of id to label. It's important to make sure that the ids are cast to a string. Let's create two variables that will contain the required mappings - label2id and id2label. We will use Python dictionaries to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608808b4-3598-4f3f-9263-4b24c873678a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "for k, v in labels.items():\n",
    "    label2id[k] = str(v)\n",
    "    id2label[str(v)] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfec984-cb9b-46b7-9827-209ba2559e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc1c24-2707-4a6c-a4cf-8aae9cb5f412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958d62b-5809-4fc7-be52-2b1d8a055eb7",
   "metadata": {},
   "source": [
    "Great! This is exactly what we need. Now let's create a variable that will contain the information about the number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ae494-a4a8-487a-9f82-b68ef9ba0d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = len(label2id)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a35a19-16da-4ab4-8804-2c82308a5009",
   "metadata": {},
   "source": [
    "Excellent! Now we have all the pieces in place to instantiate the AST model.\n",
    "\n",
    "Let's use the *ASTForAudioClassification* class to instantiate the model. We will make sure to pass the number of labels and both of the mappings. Apart from that we are adding an *ignore_mismatched_sizes* argument and setting its value to *True*. This will instantiate the model's last layer with an appropriate number of neurons, which is derived from the rest of the arguments that we passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6de841-9ef2-40b7-be6e-c3181e559314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", \n",
    "                                                  num_labels=num_labels, \n",
    "                                                  label2id=label2id, \n",
    "                                                  id2label=id2label,\n",
    "                                                  ignore_mismatched_sizes=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be178b-8eac-4dbd-9aab-908c1ce9669a",
   "metadata": {},
   "source": [
    "Great! We can see that some of the weights were \"newly initialized\". Those are the weights of the last layer. We can also see that we *\"should probably TRAIN this model\"*. This is exactly what we are about to do.\n",
    "\n",
    "If we want to train a model with the help of the ðŸ¤— library we need to create instances of two classes - TrainingArguments and Trainer. The first object tells ðŸ¤— what are the different parameters of the training process that we are about to start. The Trainer class takes those arguments along with the model, metrics that we want to compute during training, the datasets we are going to use, and the feature extractor.\n",
    "\n",
    "Feel free to inspect the documentation of both - the [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) and [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31da8f-e0b1-4fe6-b921-cf6a112390ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 4                        # variable defining number of training epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models/AST',                # directory for saving model checkpoints and logs\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,      #number of epochs\n",
    "    per_device_train_batch_size=2,          # number of examples in batch for training\n",
    "    per_device_eval_batch_size=2,           # number of examples in batch for evaluation\n",
    "    evaluation_strategy=\"epoch\",            # makes evaluation at the end of each epoch\n",
    "    learning_rate=float(5e-5),              # learning rate\n",
    "    optim=\"adamw_torch\",                    # optimizer\n",
    "    logging_steps=1,                        # number of steps for logging the training process - one step is one batch\n",
    "    load_best_model_at_end=True,            # whether to load or not the best model at the end of the training\n",
    "    metric_for_best_model=\"eval_loss\",      # claiming that the best model is the one with the lowest loss on the validation set\n",
    "    save_strategy='epoch'                   # saving is done at the end of each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d879dce-d9e5-4f02-9a21-75bd87e2fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                          # passing our model\n",
    "    args=training_args,                   # passing the above created arguments\n",
    "    compute_metrics=compute_metrics,      # passing the compute_metrics function that we imported from gdsc_eval module\n",
    "    train_dataset=train_dataset_encoded,  # passing the encoded train set\n",
    "    eval_dataset=val_dataset_encoded,     # passing the encoded validation set\n",
    "    tokenizer=feature_extractor           # passing the feature extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac005a-f814-4fb8-8881-c4f48c5c3215",
   "metadata": {},
   "source": [
    "Amazing! Now we did everything that was required to fine-tune the model. We can finally run the cell which will give us our \"version\" of the AST classifier, which is capable to distinguish different species from audio recordings. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9ff56-f633-49a0-a8b3-19a40ec41c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f74586-6f30-45fa-8837-df3021f1d3e7",
   "metadata": {},
   "source": [
    "Is it possible? We are performing better than the Random Forest model with only a fraction of data! Well, yes, that's possible, but remember that the validation set we are using here contains only 66 samples, so way less than the original set. If you want to really compare the model with the Random Forest we need to perform inference on the test set and send a submission. \n",
    "\n",
    "In the next section we will show you how to load the model from checkpoint and perform inference on the test set data.\n",
    "\n",
    "**Key insights:**\n",
    "* The ðŸ¤— models hub offers you a variety of models, BUT you should always remember to adjust them to your task - create appropriate mapping of labels to integers and specify the number of classes that you are working with\n",
    "* There is a number of parameters that define a training job - be mindful about how you are setting them and iterate over different values - this is called hyperparameter tuning\n",
    "* Fine-tuning such a big model on such a small sample is almost always a bad idea - big models require big data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81f943-e7a6-4f8c-8126-4e1324b1cd60",
   "metadata": {},
   "source": [
    "# Loading the model and doing inference on the test set\n",
    "\n",
    "If you look back at the *TrainingArguments* class you will see that we passed an *output_dir* argument that tells ðŸ¤— where to put the checkpoint with training metadata and model. We set it to *models/AST*, so let's use this directory to load the feature extractor and the model from the best checkpoint (note that this is not necessary, as we put in our *TrainingArguments* object an argument called *load_best_model_at_end* and we set it to *True*. This ensures that the variable *model* contains already the best one based on the metric of choice. We just wanted to show you how to load the model from other checkpoints in case you'd like to experiment). With ðŸ¤— library loading the checkpoint it's just a matter of two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed72b8-bb56-4b09-881b-d964e43a0da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\"models/AST/checkpoint-352\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"models/AST/checkpoint-352\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86bfec-992e-4c67-9f37-c03660b49de5",
   "metadata": {},
   "source": [
    "Cool! Now let's get the test set data. We need to preprocess them in the same way as we did for the training. Let's start with simply loading the dataset and resample the audio arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bd1f6-8c94-48af-9dc2-e3af1b7e116c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = 'data/test'\n",
    "test_dataset = load_dataset(\"audiofolder\", data_dir=test_path).get('train')\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b7717-17cc-4888-b2fa-bd901fb871f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1e9a8-d864-488a-b08a-0c066cfaa602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bf69d-5ece-4e45-8ac3-e6529f4e3206",
   "metadata": {},
   "source": [
    "As we need the predictions file to have two columns - file_name and predicted_class_id, let's take care of extracting the paths for each data point and make it a feature called \"file_name\". \n",
    "\n",
    "For this purpose we'll use the metadata information from the dataset object that we just created.\n",
    "\n",
    "So let's get the paths of the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb418f-634a-4805-9282-8390aa368985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_paths = list(test_dataset.info.download_checksums.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a93024-47cf-41a5-97fd-9c9f5e7240ed",
   "metadata": {},
   "source": [
    "Let's inspect the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38632450-9617-42fe-afc3-1b01559f992d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_paths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb031da0-211f-49ed-b509-4b099648feec",
   "metadata": {},
   "source": [
    "Great! We obtained the paths. One thing to note is that the test_paths variable contains also the metadata.csv file with file_names and labels (check it on your own!). We don't need it, so we will use a one-liner lambda function to extract only the items related to the audio files.\n",
    "\n",
    "Furthermore, we don't need the whole path - just the file names, so we will define another one-liner that gets the string after the last \"/\" character, which is exactly the file name.\n",
    "\n",
    "We will use the built-in filter and map methods that allow for applying a function on an Python iterable. With its help we will run the below defined lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77dfbd3-e588-45c0-8804-a67a92a56a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_metadata = lambda x: x.endswith(\".wav\")\n",
    "extract_file_name = lambda x: x.split('/')[-1]\n",
    "\n",
    "test_paths = list(filter(remove_metadata, test_paths))\n",
    "test_paths = list(map(extract_file_name, test_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a2149-6999-47f7-9424-f6de512c4d4a",
   "metadata": {},
   "source": [
    "Let's see if the test_paths variable contains the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bcb72-50f6-44b3-a32e-3facba68cae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_paths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69ad21-9e1c-4142-a9ac-1de52618e09d",
   "metadata": {},
   "source": [
    "Yes, we indeed have just the file names. Let's create a new column with the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edd7e2-8913-4131-bd0e-40c869213ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.add_column(\"file_name\", test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358da5f6-93b9-4638-a7f0-8ab1dfb2afad",
   "metadata": {},
   "source": [
    "Let's inspect the newly created \"file_name\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348b72c-ca57-496b-b61e-1e407780b736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce12fc-12b2-4991-9d6a-2558154d5a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df797b9d-ba7d-4cb6-a763-c20f8c91e190",
   "metadata": {},
   "source": [
    "Amazing! We almost finished preprocessing the data. The last step is to pass the audio arrays through our feature extractor and set fromat of the \"input_values\" columns from numpy to torch, so that we can safely pass the spectrogram arrays through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f979930-9633-4166-89db-17aa89fe064f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_encoded = test_dataset.map(lambda x: preprocess_audio_arrays(x, 'audio', 'array', feature_extractor), remove_columns=\"audio\", batched=True, batch_size = 2)\n",
    "test_dataset_encoded.set_format(type='torch', columns=['input_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46e0e-c9c3-4c44-b909-d8c181f41e33",
   "metadata": {},
   "source": [
    "Now let's inform the ðŸ¤— that we want to run the predicions on our GPU. To do this we need to define the *device* variable with help of the *PyTorch* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0524c-3e09-40cb-b510-262137ef5e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43121d97-728b-4998-9a62-586e95278958",
   "metadata": {},
   "source": [
    "Good, we are set up to perform the inference on the test set. Let's use the *make_predictions* function from our *gdsc_eval* modeule located in *src* directory. This time we will set the *batch_size* argument to 8, to avoid any out-of-memory issues. We are also dropping the \"input_values\" column, as we won't need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42615997-ec47-4531-809b-99aca74c98f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_encoded = test_dataset_encoded.map(lambda x: make_predictions(x['input_values'], model, device), batched=True, batch_size=8, remove_columns=\"input_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6faee-e779-46c2-beea-f0c0d94afcb9",
   "metadata": {},
   "source": [
    "Let's now create a pandas dataframe from our ðŸ¤— dataset. We should see the columns file_name and predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d177953-aa2b-4309-9ea5-ac80be298e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_encoded_df = test_dataset_encoded.to_pandas()\n",
    "test_dataset_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28f9f1-13c7-4cf7-867b-51e2afca0af1",
   "metadata": {},
   "source": [
    "Great! Now we need to save the dataframe in a csv file and we are ready to send the predictions. We will save it in the directory of our model, to have everything in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb19f7-09cf-4012-a278-e5eb2f4253a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_encoded_df.to_csv(\"models/AST/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85012d29-8b05-46dc-bf45-612958c72729",
   "metadata": {
    "tags": []
   },
   "source": [
    "And done! We have our CSV file with the predictions ready. Let's upload it via the challenge website and see our results!\n",
    "\n",
    "The score is way better than the one from Random Forest. Remember that in this tutorial we are using a much more powerful model, that was designed to work with audio data. But taking into account that the F1 metric ranges from 0 to 1, there is still some room for improvement. In the next tutorial, we will see how the model performs on the whole dataset. Then you will see what the model is really capable of! In the mean time, you can try to complete the exercises while making a coffee before the final tutorial.\n",
    "\n",
    "***\n",
    "**It is important that you name the columns exactly: **file_name** and **predicted_class_id**, otherwise your score won't appear on the leaderboard!**\n",
    "***\n",
    "\n",
    "**Exercise time:**\n",
    "\n",
    "The last exercise in this notebook is to \n",
    "* try to think how we could improve the model further apart from running it on the whole sample. What does your Data Science intuition tell you? Post your thoughts in the Team's channel and gain some recognition for your team! ðŸ˜ƒ\n",
    "* try also to use another model from the ðŸ¤— model hub. You will need to import other classes instead of ASTFeatureExtractor and ASTForAudioClassification. You will also need to change the string in the *from_pretrained* method and adjust the preprocessing. Sounds like a lot? Well, this is how we do Data Science! ðŸ˜ƒ\n",
    "\n",
    "REMINDER: After finishing your work remember to shut down the instance."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:425657697824:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
