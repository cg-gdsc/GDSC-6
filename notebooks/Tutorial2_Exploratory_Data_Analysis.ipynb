{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a1fc4e-0a1d-4e5c-8d52-fa057693a794",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial 2: EDA - Exploratory Data Analysis\n",
    "\n",
    "After setting up AWS, we can finally dive into the exciting world of Data Science. In this Jupyter Notebook, we will explore audio data using various EDA techniques. EDA is an essential step in any Data Science project, including audio data modeling. It helps us gain insights into the data's characteristics, identify patterns, and understand relationships between variables. It also allows for determining the best preparation steps for the modeling phase. A good data preparation is ofen more important than the model and hyperparamters. If you'd like to learn more about best practices and the latest developments in engineering the data to build AI systems, please visit [Data Centric AI site](https://datacentricai.org/). You will find there loads of interesting information from that domain.\n",
    "\n",
    "Throughout this tutorial, we will cover a range of EDA tools and methods, such as spectrograms, waveform plots, and statistical summaries, to visualize and summarize the audio data.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "**NOTE:** This notebook does not require a GPU instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24819050-c42e-4496-bd1d-c7204de5ed80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to import required libraries and functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafcf85-5658-4a0f-9d2b-8dc3edc9ca5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys # Python system library needed to load custom functions\n",
    "import math # module with access to mathematical functions\n",
    "import os # for changing the directory\n",
    "\n",
    "import numpy as np  # for performing calculations on numerical arrays\n",
    "import pandas as pd  # home of the DataFrame construct, _the_ most important object for Data Science\n",
    "\n",
    "from IPython.display import Audio # for listening to our insects\n",
    "from scipy.fft import fft # function to calculate Fast Fourier Transform\n",
    "\n",
    "import matplotlib.pyplot as plt  # allows creation of insightful plots\n",
    "import seaborn as sns # another library to make even more beautiful plots\n",
    "\n",
    "sys.path.append('../src') # add the source directory to the PYTHONPATH. This allows to import local functions and modules.\n",
    "# enable rendering plots under the code cell that created it\n",
    "%matplotlib inline\n",
    "\n",
    "from eda_utils import show_sampling, signal_generator, plot_random_spec, plot_spec, plot_waveform # functions to create plots for and from audio data\n",
    "from gdsc_utils import download_directory, PROJECT_DIR # function to download GDSC data from S3 bucket and our root directory\n",
    "from config import DEFAULT_BUCKET  # S3 bucket with the GDSC data\n",
    "\n",
    "os.chdir(PROJECT_DIR) # changing our directory to root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a29644-dce7-46db-afdc-1334946ed5b7",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09299792-d271-4414-814c-4d745ad0907b",
   "metadata": {},
   "source": [
    "Next we need to download the official data for the GDSC from the S3 bucket. The S3 bucket is structured as follows:\n",
    "\n",
    "```\n",
    "S3_bucket/\n",
    "    └── data/\n",
    "        |── labels.json\n",
    "        |── metadata.csv\n",
    "        └── train/\n",
    "            |── train_file_1.wav\n",
    "            |── train_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "        └── val/\n",
    "            |── val_file_1.wav\n",
    "            |── val_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "        └── test/\n",
    "            |── test_file_1.wav\n",
    "            |── test_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "    └── data_small/\n",
    "        |── labels.json\n",
    "        └── train/\n",
    "            |── train_file_1.wav\n",
    "            |── train_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "        └── val/\n",
    "            |── val_file_1.wav\n",
    "            |── val_file_2.wav\n",
    "            |── ...\n",
    "            |── metadata.csv\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f858e8-60ec-4370-b74c-8b0a13ed4fdc",
   "metadata": {},
   "source": [
    "In the official S3 bucket, you can find 2 folders:\n",
    "\n",
    "- *data* - it contains the complete dataset for the challenge.\n",
    "- *data_small* - this folder contains a small sample of the training and validation datasets. It will be utilized in the 4th tutorial, so there's no need to download it at the moment.\n",
    "\n",
    "For the purpose of this tutorial, we need to download the entire dataset, which includes the entire *data* directory. To accomplish this, we can make use of the ```download_directory``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281d5d2-77aa-4202-ac7f-554205d51ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_directory('data/', None, DEFAULT_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd11d5c-bd92-4f5f-9fcb-062783d984ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysing the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2402f5-e40f-4e91-bc11-709152202d4f",
   "metadata": {},
   "source": [
    "Let's start with loading the metadata file and printing the first few observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70e32e-c62a-41e8-9321-ee588a2a206e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/metadata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628df82a-6456-4cb2-a557-496db9c1e0ec",
   "metadata": {},
   "source": [
    "The metadata contains general information about our dataset, for each file we have:\n",
    "- <i>file_name</i> -  name of the file,\n",
    "- <i>unique_file</i> - the unique file name (context: some of the files in the unique_file column were very long, so they were cut into smaller ones. The names of those cut files are in the file_name column),\n",
    "- <i>path</i> - shows us where this specific file is located,\n",
    "- <i>species</i> - tells us what species was recorded,\n",
    "- <i>label</i> - is <i>species</i> encoded to a number,\n",
    "- <i>subset</i> - indicates if the file belongs to the train or validation dataset,\n",
    "- <i>sample_rate</i> - is a feature related to audio. It shows how many points are recorded every second,\n",
    "- <i>num_frames</i> - is the total number of samples in the recording,\n",
    "- <i>length</i> - duration of the audio file in seconds, which can be calculated by dividing <i>num_frames</i> by <i>sample_rate</i>.\n",
    "\n",
    "If some of these features are difficult to grasp, don't worry! We will do a deeper dive into sample rate, number of frames, and others later in this notebook.\n",
    "If you would like to learn more about audio processing after the session, here are some useful links:\n",
    "* [Wikipedia - Audio Signal Processing](https://en.wikipedia.org/wiki/Audio_signal_processing)\n",
    "* [Medium article on audio features](https://medium.com/analytics-vidhya/audio-data-processing-feature-extraction-science-concepts-behind-them-be97fbd587d8)\n",
    "* [Another Medium article, but this time on spectrograms and Fourier Transform](https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520)\n",
    "\n",
    "If this list seems to be a bit short, don't worry we've also included some more sources in the later parts of the notebook. We also encourage you to share on the Teams channel any other useful materials that you find on the web.\n",
    "\n",
    "But let's get back to the analysis! Now we'll focus on inspecting the dataset characteristics. For starters, let's try to inspect the numerical variables of our dataset (sample_rate, num_frames, and length) to get an idea about their distribution. The [.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) Dataframe method gives us a quick overview of the column distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e639a-987c-405d-b2d2-dbfc918d73e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"sample_rate\", \"num_frames\", \"length\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3d86c-19d2-4456-b0d1-42f0718172f1",
   "metadata": {},
   "source": [
    "Fortunately <i>sample_rate</i> is constant for all audio files. It will save us some preprocessing work!\n",
    "\n",
    "We can see that the audio length varies heavily - from 1 second to almost 12 minutes. But half of the files are between 5 and 28 seconds.\n",
    "\n",
    "Let's now inspect the most important categorical variables - file_name, unique_file, label, and subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57526cec-e13e-4b13-bc5c-23352e72aad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"file_name\", \"unique_file\", \"label\", \"subset\"]].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c442e-92ba-4a80-86de-c02868321800",
   "metadata": {},
   "source": [
    "We see that we have 2331 audio files that come from 1252 unique recordings. This is due to the decision of the data suppliers that long files will be split into smaller ones. Apart from that we notice, that there are 66 labels in the dataset and the set is split into two chunks - training and validation. Having the train-val split will be useful for the modeling phase when you will tune your model hyperparameters.\n",
    "\n",
    "**Key insights**:\n",
    "1. sampling rate of the files is constant\n",
    "2. the files vary in length, which may be a challenge in preparing the data for the modeling phase\n",
    "3. there are recordings from 66 different classes or insect species \n",
    "4. the dataset is already split into train and validation, which will make easier the evaluation and hyperparameter tuning of our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde96ab-49ae-4d60-89e6-b46372c8d627",
   "metadata": {},
   "source": [
    "Now let's visualize some of the qualities of this dataset to better understand the data we are working with. First, let's add a variable that combines <i>species</i> and <i>labels</i> columns, so it'll be easier to tag the axis on the plots we are about to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d71553-4619-4dd8-885e-1929de49168b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['species and label'] = df.apply(lambda x: f\"{x['species']} ({str(x['label'])})\", axis = 1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24093ad6-270d-408f-b2bd-e572dbdf95df",
   "metadata": {},
   "source": [
    "At the end of the DataFrame you can now see our newly created colum 'species and label'\n",
    "\n",
    "One of the main challenges when working with a large number of classes is class imbalance. It means that some of the classes have many example datapoints and some of them are underrepresented. Aggregating the data by species might help us to assess the balance of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a7270-a3cf-4447-895c-5cc5e75429a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating stats per label/species - total length of recording per class and the total number of class occurences in the dataset\n",
    "df_stats = df.groupby(['label','species and label']).agg(length = ('length', 'sum'), count = ('species', 'count')).reset_index()\n",
    "\n",
    "# Calculating average length of an audio sample\n",
    "df_stats['avg_len'] = df_stats['length']/df_stats['count']\n",
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c55687b",
   "metadata": {},
   "source": [
    "*df_stats* shows us how many seconds of recordings we have for each species, the number of different entries as well as the average length per sample. Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b709f-6b50-423f-9954-e9b5d8f31c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats = df_stats.sort_values('label')\n",
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "sns.barplot(x = df_stats['species and label'], y = df_stats['count'], color = 'royalblue')\n",
    "plt.title('Number of files per species', fontsize = 20)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af3de5-0517-4724-a7ee-12ddfe86de13",
   "metadata": {},
   "source": [
    "It looks like we have very high variance in the number of samples per species, but what about the audio length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc494-3a6c-4167-a726-98062aaa55f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats = df_stats.sort_values('label')\n",
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "sns.barplot(x = df_stats['species and label'], y = df_stats['length'], color = 'royalblue')\n",
    "plt.title('Length of files per species', fontsize = 20)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc05989-bb1a-4533-b082-f6bede3bff8a",
   "metadata": {},
   "source": [
    "The audio length also varies heavily from class to class. We have some species with just a few seconds of recordings and almost 5000 seconds (over 1 hour!) for *Grylluscampestris*, aka field crickets.\n",
    "\n",
    "The last two plots show that we are working with an imbalanced dataset. This may be a challenge when preparing a model that will perform well on all classes, because not all classes may be well represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98422a83-d39b-41fb-b0c6-3b12f99e6bd2",
   "metadata": {},
   "source": [
    "**Key insights:**\n",
    "\n",
    "* the number of examples per each class varies heavily, which means that we are working with an imbalanced dataset\n",
    "* the total audio length per class is also different from class to class, this together with the number of files gives full information about the amount of data we have per each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3595660-5ad8-4619-86b1-b512ce9963ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercises**:\n",
    "- To explore the data further, you could recreate the plots using different subsets, such as the training and validation sets.\n",
    "- The audio samples for different species exhibit varying lengths. Which species have the shortest and longest audio samples?\n",
    "- An interesting question to ask is which classes have the most and least amount of data.\n",
    "- How can the problem of class imbalance be tackled when building an AI solution? Post your thoughts on the [GDSC Teams channel](https://teams.microsoft.com/l/channel/19%3ad6ae189bbba3496abbb5f7f8939c92a4%40thread.skype/Data%2520and%2520AI%2520Questions?groupId=7d77d672-dff1-4c9f-ac55-3c837c1bebf9&tenantId=76a2ae5a-9f00-4f6b-95ed-5d33d77c4d61)! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571eb15-5a33-4609-a9fd-899a5c51903f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Foundations of audio processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954d6fe-0b5f-424b-a480-d37491bd4174",
   "metadata": {},
   "source": [
    "As we saw in the analysis above for this year's Global Data Science Challenge, we will work with audio data. It's a very specific kind of data with its features and characteristics. Before we continue exploring our data in depth let's try to understand some of the most important concepts around audio features and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184e665-65f6-44a9-83df-5a131a892e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, math.pi*6, 1000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.plot(x, y, lw = 3)\n",
    "plt.arrow(0, 0, math.pi*2, 0, lw = 4, color = 'red', head_width = 0.05, length_includes_head = True)\n",
    "plt.arrow(math.pi*2, 0,-math.pi*2, 0, lw = 4, color = 'red', head_width = 0.05, length_includes_head = True)\n",
    "\n",
    "plt.arrow(0.5*math.pi, 0, 0, 1, lw = 4, color = 'red', head_width = 0.05, length_includes_head = True)\n",
    "plt.arrow(0.5*math.pi, 1, 0, -1, lw = 4, color = 'red', head_width = 0.05, length_includes_head = True)\n",
    "plt.text(math.pi*0.5 + 0.1, 0.5, 'A - amplitude', fontsize = 'large')\n",
    "\n",
    "plt.title('Sine function as a simple sound wave', fontsize = 20)\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.text(math.pi, 0.1, 'T - period', fontsize = 'large')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43abf7-a2ea-4c92-83ef-f605dc4cdf32",
   "metadata": {},
   "source": [
    "A [sine wave](https://onlinetonegenerator.com/) is a continous *beep*, the simplest sound we can make. In the above plot, you can see a sine wave that persists for over 18 seconds. On the x-axis, you have the time domain, and on the y-axis the amplitude of the wave. The amplitude represents the intensity or volume of an audio wave, while the period represents the time it takes for the wave to complete one cycle of oscillation. \n",
    "\n",
    "As <i>amplitude</i> and <i>period</i> are quite straightforward let's try to focus on more complex features.\n",
    "\n",
    "In the analysis of the metadata table, we saw some information about the <i>sampling rate</i>. Let's see how can we understand it!\n",
    "\n",
    "Let's assume that we want to record some continuous signal that looks exactly like the below sine wave. The problem with this is that it is impossible to record and store something continuously. We cannot store infinitely many points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce384-2305-4d63-88e0-850b21aa2904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_sampling(10, 1, 1, show_signal = True, show_sampling = True, plot_sampling = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd07081-f36d-4adb-baa7-7b65413dc16f",
   "metadata": {},
   "source": [
    "The only thing we can do is to record only specific points in time. For example, let's say that our device allows us to record the signal once every 0.1 second. This would result in having 10 points (samples) per each 1 second of the signal. So the rate at which we sample the data is 10 points per second. This is exactly the definition of the sampling rate. It is the number of samples (or measurements) of the audio signal that are taken per second. It is typically measured in [Hertz (Hz)](https://en.wikipedia.org/wiki/Hertz), which is the inverse of the time unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e4ff6-256f-4abe-b193-c1fe37540387",
   "metadata": {},
   "source": [
    "From the above plot, we can see that with a low sampling rate, the reconstruction of the signal is not that precise. What if we increased the sampling rate? Let's check it on the below plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f9b0f-f820-4a26-b2e4-0e909f3a308f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_sampling(10, 100, 1, show_signal = True, show_sampling = True, plot_sampling = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e344a-29cc-403d-ac24-814b3d9836d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see, that the higher the sampling rate is, the more accurately the analog (real-world) audio signal can be converted to digital format. A higher sampling rate results in better sound quality, but also leads to larger file sizes.\n",
    "\n",
    "Great! Hopefully, by now we have a good grasp of the sampling rate. One feature connected with sampling rate is the number of frames which is sampling rate multiplied by the length of the file and gives us the total number of samples in a recording.\n",
    "\n",
    "Great, now let's move on to other features.\n",
    "\n",
    "One of the most important features of audio data is <i>frequency</i>. This is the inverse of <i>period</i> and it tells us how many cycles a signal makes per second, which means that the unit for the frequency is given also in Hertz (Hz). Let's look at simple sine functions to understand this feature better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92950ee3-cd90-421d-8929-5c49854b9b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal, time = signal_generator(2, 1000, [1,1.5], show_signals = True, show_signals_sum = False, split_plots = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9323438-7150-42e3-b8a6-1c15b8d02dbd",
   "metadata": {},
   "source": [
    "So from the above plot, we can see that the higher the frequency, the more cycles per second we have. If we were to use our newly gained knowledge about audio features we could rephrase that *the higher the frequency, the shorter the period of the signal is.* \n",
    "\n",
    "**Key insights:**\n",
    "* Audio data have different features. We've learned about amplitude, period, and frequency. Make sure that you understand them because we will build on top of that later on.\n",
    "* The sampling rate determines how good or bad is the reconstruction of the signal.\n",
    "\n",
    "So far we've inspected only separate sine waves, which were quite straightforward to analyze, but in practice, an audio recording is a sum of multiple signals. Let's consider the a signal that consists of three overlapping frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3161633-51ec-4ab1-bb5e-c9b1b0f2f82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 1 # time in seconds\n",
    "sr = 1000 # sample_rate\n",
    "freq = [4, 20, 10] # frequencies used to build signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe142be3-67c6-4e95-b9f4-726903f54008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal, time = signal_generator(t, sr, freq, show_signals = True, show_signals_sum = True, split_plots = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86ba12-7960-40d0-81ba-86e191e12390",
   "metadata": {},
   "source": [
    "An audio signal is often a combination of multiple signals, also known as components or frequencies, which vary in amplitude and frequency. While the individual sine curves (the blue plots) are very simple, the combination (the red plot) seem almost chaotic.\n",
    "\n",
    "Luckily there is a method for extracting the underlying components from a recording: signal decomposition. The process involves breaking down the audio signal into its constituent frequencies using mathematical techniques such as Fourier transform. By analyzing each frequency component separately, we can gain a better understanding of the different elements that make up an audio signal, and this can be useful in a variety of applications, such as audio processing and enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e513c-45fb-4b8b-b99c-322dd0323c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = 2 # time in seconds\n",
    "sr = 1000 # sample_rate\n",
    "freq = [7, 15, 40] # frequencies used to build signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68941c46-18d2-425d-aff7-867341cbd9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signal, time_points = signal_generator(time, sr, freq, show_signals = False, show_signals_sum = True, split_plots = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0563f-d02e-45ef-998c-36906cc368b5",
   "metadata": {},
   "source": [
    "Let's save the signal above as a <i>complex_signal</i> variable. To do so we need to sum all of the components. This will effectively create a sum of amplitudes of the components for each sample point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068731fb-569f-43d1-91c8-162aa930a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complex_signal = signal.sum(axis=0)\n",
    "complex_signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74085a91-428e-44a7-b7f2-e4311a5ee452",
   "metadata": {},
   "source": [
    "We can reconstruct the frequencies of the signals using Fourier Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea1995-8a92-4c59-9961-f60e96af4c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sample = time*sr # Number of sample points\n",
    "t = time/sr # time steps\n",
    "x = np.linspace(0.0, time, n_sample) # x-axis, running number, time has to be an integer\n",
    "y = complex_signal\n",
    "\n",
    "# plot the signal\n",
    "plt.title('Complex signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(y) \n",
    "plt.show()\n",
    "\n",
    "yf = fft(y) # perform FFT - Fast Fourier Transform\n",
    "\n",
    "# plot the graph to show frequency domain\n",
    "xf = np.linspace(0.0, time/(2.0*t), n_sample//2)\n",
    "plt.plot(xf, 2.0/n_sample * np.abs(yf[0:n_sample//2]))\n",
    "plt.xlim(0, 100)\n",
    "plt.title('Signal in frequency domain after performing FFT')\n",
    "plt.xlabel('Frequencies (0 to 100 Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xticks(np.arange(0, 100,5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b83933-3cdb-4843-99bc-232f2637c03d",
   "metadata": {},
   "source": [
    "With just a few functions, we were able to determine the frequencies of the components, which is a crucial step in the process of analyzing audio data. \n",
    "\n",
    "Unfortunately, an explanation of the details of the Fourier Transform is beyond the scope of this tutorial. For those interested in delving deeper into the mathematics behind it, we have provided a few useful resources such as:\n",
    "- a [YouTube video](https://www.youtube.com/watch?v=spUNpyF58BY&t=3s) explaining what <i>Fourier Transform</i> is,\n",
    "- a Wikipedia [article](https://en.wikipedia.org/wiki/Fourier_transform) \n",
    "- and the SciPy fftfreq [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.fftfreq.html) \n",
    "\n",
    "While it may seem unnecessary for an EDA tutorial, these lessons will prove their value as we move on to the next section. Now, we can finally return to our dataset and resume our analysis.\n",
    "\n",
    "**Key insights:**\n",
    "* Audio data usually are a mix of different signals.\n",
    "* We can analyze the data by extracting the frequencies and in that way gain a better understanding of the different elements that make up an audio signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10925e2a-d9d6-4ae2-b209-3de029d42404",
   "metadata": {},
   "source": [
    "**Excercises**:\n",
    "- To deepen your understanding of audio features, you can experiment with the plotting functions and explore the different options available.\n",
    "- An interesting question to ask is what is the minimum sampling rate needed to accurately recreate the input signal. Does this minimum rate depend on factors such as frequency or other features?\n",
    "- To further practice working with audio data, you can create a complex audio sample and attempt to extract the individual component frequencies using signal decomposition techniques such as Fourier Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792102b-23af-4ba4-a86b-eeff5c1ccd65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Advanced audio processing: Waveform and spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93983d8-22c3-4473-86cd-b91de474ce60",
   "metadata": {},
   "source": [
    "A waveform represents the shape of the audio signal over time. It is a graphical representation of the amplitude of the audio signal on the vertical axis versus time on the horizontal axis. A waveform provides a visual representation of the audio signal and allows us to identify patterns, variations, and trends in the signal. It can be useful in analyzing the characteristics of an audio signal, such as its volume, pitch, and duration.\n",
    "\n",
    "Let's take a look at one of our insect recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3722cac-d214-4553-ad20-31ed48552a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_path = 'data/train/Chorthippusbiguttulus_XC751834-dat031-007_edit3.wav'\n",
    "plot_waveform(example_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c5c62-74e3-436d-a06f-67bd66eaedcb",
   "metadata": {},
   "source": [
    "At first glance we can see that the file starts with a silence and then some noise starts to appear and get louder and louder until it stops at around 3rd/4th second. Then the noise appears another few times.\n",
    "\n",
    "If we zoom in to the first 0.01 second of the recording we will see, that it is a complex signal build of multiple componets - exactly what we've previously discussed. To zoom in you can adjust the second parameter of the plotting function to display only the first few seconds of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303507fe-167e-4d52-9554-dfe6567575b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_waveform(example_path, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c524f-da8e-43f9-a062-2dde4de2d7da",
   "metadata": {},
   "source": [
    "A spectrogram on the other hand is a visual representation of the frequency content of an audio signal as it varies with time. It is a 2D plot that shows how the energy of different frequencies changes over time in an audio signal. In a spectrogram, the horizontal axis represents time, the vertical axis represents frequency, and the color intensity represents the energy or amplitude of the frequencies. To obtain a spectrogram we need the **Fourier Transform** (about which we learned a bit earlier) to decompose the audio signal into its consituent frequency components across time.\n",
    "\n",
    "The spectrogram is the most useful way of plotting audio as it gives us all 3 important features:\n",
    "- time,\n",
    "- frequency,\n",
    "- amplitude (volume)\n",
    "\n",
    "Let's plot an audio sample from our dataset and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983f74b-e22f-4402-b3d5-5b72453058b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spec([example_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70305abc-f201-44e1-9bf0-2645fcf86d18",
   "metadata": {},
   "source": [
    "The frequency ranges from 0 to 22050 Hz and it's connected to the limit of human hearing, which is approximately 20 kHz. The value of 22050 Hz is also connected to our sampling rate which (if you recall) is twice as high and is equal to 44100 Hz. The reason why we sample data with the frequency of 44100 Hz is connected to so called [<i>Nyquist frequency</i>](https://en.wikipedia.org/wiki/Nyquist_frequency), which states that to accurately represent a signal, the sampling rate must be at least twice the highest frequency present in the signal.\n",
    "\n",
    "Let's check if we can hear what we see on the spectrogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750f196-9877-4840-ac3c-3fd3a9ed857d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Audio(example_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71106d-7cbc-48c1-8b4e-69fb23104c7b",
   "metadata": {},
   "source": [
    "We can well hear that the insect is relatively quiet for the first small amount of the recording and then begins to make rhythmic louder sounds. The recording ends with a bit different noise that ends quickly. All of this can also be seen on the spectrogram. <br>\n",
    "\n",
    "Do you think insects of the same species make similar sounds? Let's try to find out. We pick and plot four samples from the same species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e107db1-c7ae-44b8-95cc-dcd2b3b82d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = list(df[df['label']==50].sample(4)['path'])\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0061900-fa02-48d2-ac33-8b2199f70b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spec(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d006ce-adde-4a04-9565-43796cabbb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Audio(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821a309-3c44-4718-b533-41208ecef386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Audio(paths[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a0ef7-37d3-4497-ada0-6917738fc18a",
   "metadata": {},
   "source": [
    "There are some visible similarities between the spectrograms, which are also clearly audible. \n",
    "\n",
    "Let's also look at the spectrograms of different species and check if we can also *see* differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1f911-b63b-4205-882c-54a80f0dcc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_random_spec(df, labels = [10, 20, 30, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27ddb4-b55c-4b4e-bb63-8894f5b5f39f",
   "metadata": {},
   "source": [
    "There are some obvious differences between the recordings. Different classes produce different spectrograms.\n",
    "\n",
    "Spectrograms are essential tools for analyzing audio signals, as they provide a visual representation of how the frequency content of the audio changes over time. By displaying audio as a two-dimensional image, spectrograms make it easier to identify patterns and features in complex audio signals. Moreover, many audio classification models rely on spectrograms as their input data, making them a crucial component of audio analysis and machine learning. You will learn more about it in the next tutorials.\n",
    "\n",
    "**Key insights:**\n",
    "* waveform is the plot of the amplitude of a signal over time\n",
    "* spectrograms give a rich signal representation by combining the time and frequency domains\n",
    "* different classes have different looking spectrograms, while the same clsses have similarly looking spectrograms. This makes it an useful audio representation and may be an input for our future models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b8ad5-f455-4bbb-91b4-23a2d20d3701",
   "metadata": {},
   "source": [
    "**Exercises:** \n",
    "- Generate spectrograms and listen to the audio samples. Can you differentiate between the species based on their sound patterns?\n",
    "- Can you observe any resemblances within a species? Perhaps certain species produce very similar sounds?\n",
    "- Attempt to identify the predominant frequencies for particular species. Do they emit sounds in higher or lower frequencies?\n",
    "- Analyze some of the waveforms generated from the dataset by decomposing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d983f-7d32-4c0b-92fc-b762fc24284d",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the next tutorial, we'll prepare a baseline model with the use of the knowledge we gathered from this tutorial. You will also send your first submission which will put your team on the leaderboard!\n",
    "\n",
    "**REMINDER: After finishing your work remember to shut down the instance.**"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:425657697824:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
